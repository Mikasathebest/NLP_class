{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Can you come up out 3 sceneraies which use AI methods?\n",
    "Ans: {语音识别 自动驾驶 照片反“PS”}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. How do we use Github; Why do we use Jupyter and Pycharm;\n",
    "Ans: {登录GitHub 查看他人repo，包括代码文档等等，可以进行star warch  folk，可以创建自己的repo，供其他人查看；\n",
    "       jupyter用来展示，运行小段代码，方便查看代码运行结果，pycharm可以进行项目开发}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. What's the Probability Model?\n",
    "Ans: A probability model is a mathematical representation of a random phenomenon. It is defined by its sample space, events within the sample space, and probabilities associated with each event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Can you came up with some sceneraies at which we could use Probability Model?\n",
    "Ans:比如：\n",
    "古典概率模型，掷骰子，6种数字出现的概率是等可能的。\n",
    "条件概率，A事件发生的条件下，B事件发生的概率 \n",
    "$$ P(B|A) = \\frac{P(AB)}{P(A)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?\n",
    "Ans:a.第一课中，概率主要用于：1.描绘单个词汇出现的概率 2.两个词接连出现的概率 3.由（2）得到句子出现的概率\n",
    "b.基于语句分析和模式匹配的难点在于：输入变化时，需要重写程序"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. What's the Language Model;\n",
    "Ans:建立一种概率模型，可以评估语言出现的可能性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Can you came up with some sceneraies at which we could use Language Model?\n",
    "Ans:比如语音助手（天猫精灵、Siri），作出回答时，需要使用语言模型对回答进行评估。\n",
    "或者，AI翻译时，对翻译的语句进行判断。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. What's the 1-gram language model;\n",
    "Ans:单词之间彼此独立，语句的概率由每个单词出现的概率相乘得到。即：$$ P(ABCD) = P(A)*P(B)*P(C)*P(D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. What's the disadvantages and advantages of 1-gram language model;\n",
    "Ans:没有考虑实际情况中，单词的出现是前后相关的，并非独立存在的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. What't the 2-gram models;\n",
    "Ans:2-gram是指以前后两个单词连续出现的概率，评估句子or文本的可能性。即：$$ P(ABCD)= P(A|B)*P(B|C)*P(C|D)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 设计你自己的句子生成器\n",
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。\n",
    "\n",
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 ,\n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法:\n",
    "\n",
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jeff = '''\n",
    "Jeff = 打招呼 标点 自己 想法\n",
    "打招呼 = 你好 | 您好 | Hi\n",
    "标点 = ， | ！ | 。 | ...\n",
    "自己 = 我 | 我们\n",
    "想法 = 想随便转转 | 想买东西 | 想卖东西 | 没事，打扰了 \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Two_B = '''\n",
    "Two_B = 同伴 标点 想法\n",
    "同伴 = 9s | 奈兹 | None\n",
    "标点 = ， | ..... |  None\n",
    "想法 = ... | 活下去 | 做自己 | 没什么\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。\n",
    "\n",
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "choice = random.choice\n",
    "def generate(gram, target):\n",
    "    if target not in gram:\n",
    "        return target\n",
    "    expand = [generate(gram, t) for t in choice(gram[target])]\n",
    "    return ''.join([e  for e in expand if e != 'null'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grammar(grammar_str, split='=>', line_split= '\\n'):\n",
    "    grammar = {}\n",
    "    \n",
    "    for line in grammar_str.split(line_split):\n",
    "        if not line.strip(): continue\n",
    "\n",
    "        exp, stmt = line.split(split)\n",
    "        grammar[exp.strip()] =  [s.split() for s in stmt.split('|')]\n",
    "    #print('create_grammar......Done')\n",
    "    return grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'您好，我想买东西'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate(create_grammar(Jeff,split= '='),target='Jeff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n(gram, target, n= 3):\n",
    "    s= []\n",
    "    for sen in [generate(gram = gram, target = target) for i in range(n)]:\n",
    "        s.append(sen+'\\n')\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['奈兹........\\n', 'None，做自己\\n', '9s，活下去\\n', 'None，活下去\\n', 'None........\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(create_grammar(Two_B,split= '='),target='Two_B',n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['你好。我想随便转转\\n',\n",
       " 'Hi...我们想随便转转\\n',\n",
       " '你好...我们想随便转转\\n',\n",
       " 'Hi...我们没事，打扰了\\n',\n",
       " '你好！我想卖东西\\n']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_n(create_grammar(Jeff,split= '='),target='Jeff',n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 使用新数据源完成语言模型的训练\n",
    "按照我们上文中定义的prob_2函数，我们更换一个文本数据源，获得新的Language Model:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "选择:豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'H:/NLP/NLP_class/datasource/movie_comments.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv(filename, encoding='utf-8',low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>link</th>\n",
       "      <th>name</th>\n",
       "      <th>comment</th>\n",
       "      <th>star</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京意淫到了脑残的地步，看了恶心想吐</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>中二得很</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>“犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>https://movie.douban.com/subject/26363254/</td>\n",
       "      <td>战狼2</td>\n",
       "      <td>脑子是个好东西，希望编剧们都能有。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                        link name  \\\n",
       "0  1  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "1  2  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "2  3  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "3  4  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "4  5  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "5  6  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "6  7  https://movie.douban.com/subject/26363254/  战狼2   \n",
       "\n",
       "                                             comment star  \n",
       "0                                 吴京意淫到了脑残的地步，看了恶心想吐    1  \n",
       "1  首映礼看的。太恐怖了这个电影，不讲道理的，完全就是吴京在实现他这个小粉红的英雄梦。各种装备轮...    2  \n",
       "2  吴京的炒作水平不输冯小刚，但小刚至少不会用主旋律来炒作…吴京让人看了不舒服，为了主旋律而主旋...    2  \n",
       "3                      凭良心说，好看到不像《战狼1》的续集，完虐《湄公河行动》。    4  \n",
       "4                                               中二得很    1  \n",
       "5                        “犯我中华者，虽远必诛”，吴京比这句话还要意淫一百倍。    1  \n",
       "6                                  脑子是个好东西，希望编剧们都能有。    2  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = content['comment'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'犯我中华者虽远必诛，是有多无脑才信这句话。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall('\\w+',string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_clean = [''.join(token(str(a))) for a in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'犯我中华者虽远必诛是有多无脑才信这句话'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_clean[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('article_movie','w',encoding='utf-8') as f:\n",
    "    for a in article_clean:\n",
    "        f.write(a+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string): \n",
    "    return list(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\60230\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.787 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['犯', '我', '中华', '者', '虽远必', '诛', '是', '有', '多', '无脑', '才', '信', '这句', '话']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut(article_clean[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "261497"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i,line in enumerate(open('article_movie',encoding='utf-8')):\n",
    "    #if i % 1000 == 0:\n",
    "     #   print(i)\n",
    "    #if i > 5000: break\n",
    "  \n",
    "    TOKEN += cut(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from functools import reduce\n",
    "from operator import add, mul\n",
    "reduce(add, [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = Counter(TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 328262),\n",
       " ('\\n', 261497),\n",
       " ('了', 102420),\n",
       " ('是', 73106),\n",
       " ('我', 50338),\n",
       " ('都', 36255),\n",
       " ('很', 34712),\n",
       " ('看', 34022),\n",
       " ('电影', 33675),\n",
       " ('也', 32065)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_2_gram = [''.join(TOKEN[i:i+2]) for i in range(len(TOKEN[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['胡闹\\n',\n",
       " '\\n吴京',\n",
       " '吴京的',\n",
       " '的炒作',\n",
       " '炒作水平',\n",
       " '水平不输',\n",
       " '不输冯小刚',\n",
       " '冯小刚但小刚',\n",
       " '但小刚至少',\n",
       " '至少不会']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TOKEN_2_gram[50:60]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count_2 = Counter(TOKEN_2_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pro_2(word1, word2):\n",
    "    if word1+word2 in word_count_2: return word_count_2[word1+word2] / len(TOKEN_2_gram)\n",
    "    else:\n",
    "        return 1/len(TOKEN_2_gram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.417848532600644e-07"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_2('不会','的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0007338259458294611"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pro_2('我','的')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probability(sentence):\n",
    "    words = cut(sentence)\n",
    "    \n",
    "    sentence_pro = 1\n",
    "    for i, word in enumerate(words[:-1]):\n",
    "        next_ = words[i+1]\n",
    "        \n",
    "        probability = pro_2(word, next_)\n",
    "        \n",
    "        sentence_pro *= probability\n",
    "        \n",
    "    return sentence_pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.208924266300322e-07"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability('我吃饭')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.528893478766973e-13"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_probability('我吃手机')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence:您好！我们想买东西 with Pro:\t3.2691192260301303e-31\n",
      "sentence:您好！我想随便转转 with Pro:\t4.012100868309706e-31\n",
      "sentence:Hi...我们想卖东西 with Pro:\t2.157531512667192e-26\n",
      "sentence:你好。我想随便转转 with Pro:\t4.012100868309706e-31\n",
      "sentence:您好...我没事，打扰了 with Pro:\t1.7373074797812047e-40\n",
      "sentence:Hi，我没事，打扰了 with Pro:\t1.7373074797812047e-40\n",
      "sentence:您好。我想买东西 with Pro:\t2.888712625182988e-29\n",
      "sentence:你好！我们想卖东西 with Pro:\t2.157531512667192e-26\n",
      "sentence:您好...我没事，打扰了 with Pro:\t1.7373074797812047e-40\n",
      "sentence:您好，我想卖东西 with Pro:\t1.9064733002841008e-24\n"
     ]
    }
   ],
   "source": [
    "for sen in [generate(create_grammar(Jeff,split= '='),target='Jeff') for i in range(10)]:\n",
    "    print('sentence:{} with Pro:\\t{}'.format(sen, get_probability(sen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 获得最优质的的语言\n",
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成n个句子，并能选择一个最合理的句子:\n",
    "\n",
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_best(gram, language_model=get_probability, num=50):\n",
    "    #生成语句\n",
    "    sentence = []\n",
    "    sentence = generate_n(create_grammar(gram,split= '='),target='host',n=num)\n",
    "    #根据语言模型，判断语句\n",
    "    pro,p1 = [],[]\n",
    "    for line in sentence:\n",
    "        p1 = language_model(line)\n",
    "        pro.append((line,p1))\n",
    "        #pro.append(p1)\n",
    "    \n",
    "    pro = sorted(pro,key=lambda x:x[1],reverse=True)\n",
    "    \n",
    "    return pro[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('你好我是5号,您需要玩一玩赌博吗？\\n', 7.980551372579765e-75)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_best(gram=host, language_model = get_probability, num = 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
